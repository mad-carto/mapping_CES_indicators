{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# S1 - Geosocial Media Data Processing & Language Model Training\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the Supplementary Material provided for the paper\n",
    "_Mapping indicators of cultural ecosystem services use in urban green spaces based on text classification of geosocial media data_ published in the Ecosystem Services: Science, Policy & Practice Journal. This includes the HTML conversions of a series of three Jupyter notebooks as follows: \n",
    "\n",
    "    1. S1_GSM_Data_Processing&LanguageModelTraining.html\n",
    "    2. S2_GSM_Data_TextClassification.html\n",
    "    3. S3_Generate_ChiValueExpectationSurface.html\n",
    "https://doi.org/10.1016/j.ecoser.2022.101508"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook the following processes are addressed:\n",
    "\n",
    "    1. Text Normalization of Instagram & Flickr geotagged posts\n",
    "    2. Creation of geosocial media corpus required to train a word2vec model\n",
    "    2. Language Model Training - word2vec model training using the Gensim topic modeling toolkit (Řehůřek & Sojka, 2010)\n",
    "    \n",
    "**Input data**:\n",
    " - collected Instagram and Flickr textual annotations (.csv file)\n",
    " - list of stop words (.txt file)\n",
    "\n",
    "**Output data**:\n",
    " - normalized Instagram and Flickr annotations in English and German (.csv file)\n",
    " - word2vec language model trained on geosocial media corpus (.model file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Last update: 2023-01-02**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from IPython.display import clear_output, display, Markdown\n",
    "date = dt.date.today()\n",
    "display(Markdown(f'**Last update: {date}**'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - gensim - version 4.1.2\n",
    " - nltk - version 3.6.5\n",
    " - pandas - version 1.3.3\n",
    " - polyglot - version 16.7.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# NLP Tools for text normalization\n",
    "import ftfy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from polyglot.detect import Detector\n",
    "# dissable warnings from polyglot module\n",
    "import logging\n",
    "logging.getLogger(\"polyglot\").setLevel(logging.CRITICAL)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Tools to work with WordEmbeddings\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = Path.cwd() / '01_Input'\n",
    "OUTPUT = Path.cwd() / '02_Output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parse the collected geosocial media posts into a pandas dataframe\n",
    " - all Flickr and Instagram posts were saved in a single .csv file with an _origin_id_ column to differentiate between the two platforms (we encoded Instagram with the value 1 and Flickr with 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = INPUT/'01_FlickrInstagram_Unfiltered.csv'\n",
    "cols = ['origin_id', 'latitude', 'longitude', 'user_guid', 'post_date','tags','post_title','post_body']\n",
    "dtypes={'origin_id': str,'latitude': float, 'longitude': float, 'user_guid': str, 'post_date': str,'tags':str,'post_title':str,'post_body':str}\n",
    "\n",
    "df_dd = pd.read_csv(input_file,usecols=cols, dtype=dtypes,  encoding = \"UTF-8\")\n",
    "print(len(df_dd), \"GSM posts collected in total for the city of Dresden out of which\",\n",
    "      len(df_dd[df_dd.origin_id=='1']),'were shared on Instagram and', \n",
    "      len(df_dd[df_dd.origin_id=='2']),'were shared on Flickr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Geosocial media Text Normalization & Corpus Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The textual metadata collected from Instagram and Flickr is represented by the post's title, body (caption) and the tags the user added to the post. \n",
    "We concatenate the _tags_, _posts_title_ and _post_body_  in a new column called _post_text_, which is regarded as a constituent document of the geosocial media corpus that is being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dd[\"post_text\"] = df_dd[\"tags\"].fillna('') + ' ' + df_dd[\"post_title\"].fillna('') + ' ' + df_dd[\"post_body\"].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text normalization and language detection of geosocial media posts by performing the following tasks:\n",
    " - set all words to lowercase\n",
    " - remove the mentions (@username)\n",
    " - remove all punctuations, including the question and exclamation marks\n",
    " - remove the URLs as they do not contain useful information (http:\\\\ as well as www.)\n",
    " - remove all html tags (<>)\n",
    " - remove digits\n",
    " - remove multiple characters (people sometimes repeat characters for added emphasis as in e.g. \"haaaaaaaappy\")\n",
    " - select only English and German posts for further analysis\n",
    " - remove stopwords (English and German stopwords and all the words from the SortOutAlways_InStr files, which include irrelevant words for our analysis representing mostly technical terms associated with photo cameras originating from Flickr content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "\n",
    "    @staticmethod\n",
    "    def fix_bad_unicode (input_text):\n",
    "        return ftfy.fixes.fix_encoding(input_text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_mentions(input_text):\n",
    "        return re.sub(r'[^\\s]*@[^\\s]*','',input_text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_urls(input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "        return re.sub(r'www?.\\S+','',input_text)\n",
    "        return re.sub(r'ww?.\\S+','',input_text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_html_tags(input_text):\n",
    "        return re.sub(r'<.+>[^<>]*</.>', '',input_text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_letter_numbers_combos (input_text):\n",
    "        return re.sub(r'[^\\s0-9]+[0-9]+', '', input_text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_punctuation(input_text):\n",
    "    # Make translation table\n",
    "        input_text = re.sub(' +',' ',input_text)\n",
    "        punct = '!\"$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_digits(input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_multiple_characters(input_text):\n",
    "        return''.join(''.join(s)[:2] for _, s in itertools.groupby(input_text))\n",
    "    \n",
    "    @staticmethod\n",
    "    def lang_detect(input_text):\n",
    "        #language detection with the polyglot library\n",
    "        l = Detector(input_text, quiet=True).language.code\n",
    "        if (l=='de'or l=='en'):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_stopwords(input_text, stopwords_list):\n",
    "        text = input_text.replace('#','')\n",
    "        words = text.split(' ') \n",
    "        clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 1] #remove single characters\n",
    "        return \" \".join(clean_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the corpus list\n",
    "corpus_list = []\n",
    "\n",
    "#initialize dataframe for the normalized geosocial media posts\n",
    "normalized_df = pd.DataFrame({'origin_id': [],'latitude':[],'longitude':[], 'user_guid':[],'post_date':[],\n",
    "                                 'tags':[],'post_title':[],'post_body':[], 'post_text':[]})\n",
    "\n",
    "#load stopwords lists \n",
    "with open(INPUT/'SortOutAlways_inStr.txt', 'r') as f:\n",
    "            list_out = f.read().splitlines()\n",
    "punctuation_list= [s for s in string.punctuation]\n",
    "alphabet_string = string.ascii_lowercase\n",
    "single_letters = list(alphabet_string)\n",
    "stopwords_list = stopwords.words('english') + stopwords.words('german') + list_out + punctuation_list + single_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x=0\n",
    "total_records = len(df_dd)\n",
    "for index, row in df_dd.iterrows():\n",
    "    x+=1\n",
    "    msg_text = (\n",
    "        f'Processed records: {x} ({x/(total_records/100):.2f}%). ')\n",
    "    if x % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(msg_text)\n",
    "    text = row['post_text']\n",
    "    if len(text) > 2:\n",
    "        text = Preprocessing.fix_bad_unicode(text)\n",
    "        text = text.lower()\n",
    "        text = Preprocessing.remove_mentions(text)\n",
    "        text = Preprocessing.remove_urls(text)\n",
    "        text = Preprocessing.remove_html_tags(text)\n",
    "        text = Preprocessing.remove_letter_numbers_combos(text)\n",
    "        text = Preprocessing.remove_digits(text)\n",
    "        text = Preprocessing.remove_punctuation(text)\n",
    "        text = Preprocessing.remove_multiple_characters(text)\n",
    "        if Preprocessing.lang_detect(text) == True:\n",
    "            text = Preprocessing.remove_stopwords(text, stopwords_list)\n",
    "            if (len(text)>0):\n",
    "                corpus_list.append(text)\n",
    "                normalized_df = normalized_df.append({'origin_id': row['origin_id'],\n",
    "                                                      'latitude' : row['latitude'],\n",
    "                                                      'longitude' : row ['longitude'],\n",
    "                                                      'user_guid' : row['user_guid'],\n",
    "                                                      'post_date': row['post_date'],\n",
    "                                                      'tags': row['tags'],\n",
    "                                                      'post_title': row['post_title'],\n",
    "                                                      'post_body': row['post_body'],\n",
    "                                                      'post_text' : text}, ignore_index=True)       \n",
    "\n",
    "\n",
    "# final status\n",
    "clear_output(wait=True)\n",
    "print(msg_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the normalized geosocial media posts as .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe as .csv\n",
    "output_file = OUTPUT/'Normalized_GeosocialMediaData.csv'\n",
    "normalized_df.to_csv(output_file, encoding= \"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Save the Corpus as .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputCorpus = OUTPUT/'corpus_GeosocialMedia.txt'\n",
    "with open(outputCorpus, 'w') as f:\n",
    "    for item in corpus_list:\n",
    "        f.writelines(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training word2vec language model and Word Embeddings on the geosocial social media corpus previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read corpus file\n",
    "corpusFile = OUTPUT/'corpus_GeosocialMedia.txt'\n",
    "corpus = [line.rstrip('\\n').split(' ') for line in open(corpusFile)]\n",
    "\n",
    "#train word2vec model on geosocial media corpus \n",
    "model_w2v = Word2Vec(sentences=corpus,\n",
    "                     corpus_file=None,\n",
    "                     size=300, \n",
    "                     alpha=0.025, \n",
    "                     window=5,\n",
    "                     min_count=5,\n",
    "                     max_vocab_size=None,\n",
    "                     sample=0.001,\n",
    "                     seed=1,\n",
    "                     workers=4,\n",
    "                     min_alpha=0.0001,\n",
    "                     sg=1,\n",
    "                     hs=0,\n",
    "                     negative=5,\n",
    "                     ns_exponent=0.75,\n",
    "                     iter=15,\n",
    "                     null_word=0,\n",
    "                     trim_rule=None,\n",
    "                     sorted_vocab=1,\n",
    "                     batch_words=10000,\n",
    "                     compute_loss=False,\n",
    "                     callbacks=(),\n",
    "                     max_final_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model_file = OUTPUT/'word2vec_GeosocialMedia.model'\n",
    "model_w2v.save(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### References\n",
    "\n",
    "1. Mikolov, T., Chen, K., Corrado, G., Dean, J., 2013. Efficient Estimation of Word Representations in Vector Space. ICLR Workshop. https://doi.org/10.48550/arXiv.1301.3781\n",
    "2. Řehůřek, R., Sojka, P., 2010. Software Framework for Topic Modelling with Large Corpora. In Proc. of the LREC 2010 Workshop on New Challenges for NLP Frameworks, 45–50. Valletta, Malta: ELRA.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
